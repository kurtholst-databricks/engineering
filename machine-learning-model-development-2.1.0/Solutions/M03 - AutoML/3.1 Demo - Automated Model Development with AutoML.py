# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC <div style="text-align: center; line-height: 0; padding-top: 9px;">
# MAGIC   <img src="https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png" alt="Databricks Learning">
# MAGIC </div>
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC # Automated Model Development with AutoML
# MAGIC
# MAGIC In this demo, we will demonstrate how to initiate AutoML experiments both through the user-friendly AutoML UI and programmatically using the AutoML API. When using the API, we will demonstrate some custom functionalities such as feature table integration and custom split ratios for train, validation and test.
# MAGIC
# MAGIC **Learning Objectives:**
# MAGIC
# MAGIC *By the end of this demo, you will be able to:*
# MAGIC
# MAGIC * Start an AutoML experiment via the AutoML UI.
# MAGIC
# MAGIC * Start an AutoML experiment via the AutoML API.
# MAGIC
# MAGIC * Open and edit a notebook generated by AutoML.
# MAGIC
# MAGIC * Identify the best model generated by AutoML based on a given metric.
# MAGIC
# MAGIC * Modify the best model generated by AutoML.
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC ## Requirements
# MAGIC
# MAGIC Please review the following requirements before starting the lesson:
# MAGIC
# MAGIC * To run this notebook, you need to use one of the following Databricks runtime(s): **16.2.x-cpu-ml-scala2.12**

# COMMAND ----------

# MAGIC %md
# MAGIC ## REQUIRED - SELECT CLASSIC COMPUTE
# MAGIC Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.
# MAGIC Follow these steps to select the classic compute cluster:
# MAGIC 1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.
# MAGIC 1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:
# MAGIC   - In the drop-down, select **More**.
# MAGIC   - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.
# MAGIC   
# MAGIC **NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:
# MAGIC 1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.
# MAGIC 1. Find the triangle icon to the right of your compute cluster name and click it.
# MAGIC 1. Wait a few minutes for the cluster to start.
# MAGIC 1. Once the cluster is running, complete the steps above to select your cluster.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Classroom Setup
# MAGIC
# MAGIC Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:

# COMMAND ----------

# MAGIC %run ../Includes/Classroom-Setup-3.1

# COMMAND ----------

# MAGIC %md
# MAGIC **Other Conventions:**
# MAGIC
# MAGIC Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:

# COMMAND ----------

print(f"Username:          {DA.username}")
print(f"Catalog Name:      {DA.catalog_name}")
print(f"Schema Name:       {DA.schema_name}")
print(f"Working Directory: {DA.paths.working_dir}")
print(f"User DB Location:  {DA.paths.datasets}")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Prepare Data
# MAGIC
# MAGIC For this demonstration, we will utilize a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including gender, as well as internet subscription details such as subscription plans and payment methods.
# MAGIC
# MAGIC A table with all features is already created for you.
# MAGIC
# MAGIC **Table name: `customer_churn`**
# MAGIC
# MAGIC To get started, execute the code block below and review the dataset schema.

# COMMAND ----------

churn_data = spark.sql("SELECT * FROM customer_churn")
display(churn_data)

# COMMAND ----------

# MAGIC %md
# MAGIC ## AutoML Experiment with UI
# MAGIC
# MAGIC Databricks AutoML supports experimentation via the UI and the API. Thus, **in the first section of this demo we will demonstrate how to create an experiment using the UI**. Then, show how to create the same experiment via the API.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### Create AutoML Experiment
# MAGIC
# MAGIC Let's initiate an AutoML experiment to construct a baseline model for predicting customer churn. The target field for this prediction will be the `Churn` field.
# MAGIC
# MAGIC Follow these step-by-step instructions to create an AutoML experiment:
# MAGIC
# MAGIC 1. Navigate to the **Experiments** section in Databricks.
# MAGIC
# MAGIC
# MAGIC 2. Click on **Start training** Under **Classification**.
# MAGIC
# MAGIC   <!-- ![create_experiment](../images/automl-create-experiment-v1.png) -->
# MAGIC   ![automl-create-experiment-v1](files/images/machine-learning-model-development-2.1.0/automl-create-experiment-v1.png)
# MAGIC
# MAGIC 3. Choose a cluster to execute the experiment.
# MAGIC
# MAGIC 4. Select the **catalog > database > `customers_churn` table**, which was created in the previous step, as the input training dataset.
# MAGIC
# MAGIC 5. Specify **`Churn`** as the prediction target.
# MAGIC
# MAGIC 6. Deselect the **CustomerID** field as it's not needed as a feature.
# MAGIC
# MAGIC 7. In the **Advanced Configuration** section, set the **Timeout** to **5 minutes**.
# MAGIC
# MAGIC 8. Enter a name for your experiment. Let's enter `Churn_Prediction_AutoML_Experiment` as experiment name.
# MAGIC
# MAGIC <!-- ![input_fields](../images/automl-input-fields-v1.png) -->
# MAGIC ![automl-input-fields-v1](files/images/machine-learning-model-development-2.1.0/automl-input-fields-v1.png)
# MAGIC
# MAGIC **Optional Advanced Configuration:**
# MAGIC <img src ="https://files.training.databricks.com/images/automl-advanced-configuration-optional-v1.png"> 
# MAGIC - You have the flexibility to choose the **evaluation metric** and your preferred **training framework**.
# MAGIC
# MAGIC - If your dataset includes a timeseries field, you can define it when splitting the dataset.
# MAGIC
# MAGIC 9. Click on **Start AutoML**.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ### View the Best Run
# MAGIC
# MAGIC Once the experiment is finished, it's time to examine the best run:
# MAGIC
# MAGIC 1. Access the completed experiment in the **Experiments** section.
# MAGIC <img src = "https://files.training.databricks.com/images/automl-completed-experiment-v1.png" width = 1000>
# MAGIC
# MAGIC 2. Identify the best model run by evaluating the displayed **metrics**. Alternatively, you can click on **View notebook for the best model** to access the automatically generated notebook for the top-performing model.
# MAGIC <img src ="https://files.training.databricks.com/images/automl-metrics-v1.png" width = 1000>
# MAGIC
# MAGIC 3. Utilize the **Chart** tab to compare and contrast the various models generated during the experiment.
# MAGIC
# MAGIC You can find all details for the run  on the experiment page. There are different columns such as the framework used (e.g., `Scikit-Learn`, `XGBoost`), evaluation metrics (e.g., `Accuracy`, `F1 Score`), and links to the corresponding notebooks for each model. This allows you to make informed decisions about selecting the best model for your specific use case.

# COMMAND ----------

# MAGIC %md
# MAGIC ### View the Notebook
# MAGIC
# MAGIC ####**Instruction for viewing the notebook of the best run:**
# MAGIC
# MAGIC
# MAGIC
# MAGIC + **Click on the `"View notebook for best model"` link.**
# MAGIC
# MAGIC + **Review the notebook that created the best model.**
# MAGIC
# MAGIC <img src ="https://files.training.databricks.com/images/automl-best-model-notebook-v1.png" width= 1000>
# MAGIC
# MAGIC
# MAGIC + **Edit the notebook as required.**
# MAGIC     + Identify the best model generated by AutoML based on a given metric and modify it as needed. The best model details, including the associated run ID, can be found in the MLflow experiment logs. Use the run ID to load the best model, make modifications, and save the modified model for deployment or further use.

# COMMAND ----------

# MAGIC %md
# MAGIC ## AutoML Experiment with API
# MAGIC
# MAGIC In the previous section, we created an AutoML experiment using the user interface (UI) with basic functionalities. AutoML also supports advanced functionalities, such as **feature table integration** and **custom data split ratios**, which can enhance model performance and flexibility.
# MAGIC
# MAGIC In this section, we will utilize the AutoML API to create an experiment incorporating these advanced features. By leveraging the API, we gain greater control over the experiment's configuration, enabling the customization of feature inputs and the specification of data splitting strategies.

# COMMAND ----------

# MAGIC %md
# MAGIC ### Set Features Table
# MAGIC
# MAGIC AutoML supports the use of feature tables as input. During setup, a feature table (**`customer_churn_features`**) is created. In this section, we will utilize this feature table during model training. 
# MAGIC
# MAGIC

# COMMAND ----------

features_table_path = f"{DA.catalog_name}.{DA.schema_name}.customer_churn_features"

# View features tables
display(spark.sql(f"SELECT * FROM {features_table_path}"))

# Define the feature store lookups
feauture_store_lookups = [
    {
        "table_name": features_table_path,
        "lookup_key": ["CustomerID"]
    }
]

# COMMAND ----------

# MAGIC %md
# MAGIC ### Set Custom Split - Random Split
# MAGIC
# MAGIC If you prefer AutoML to split the dataset with a different ratio than **the default 60:20:20**, you can create a new column in your dataset with the desired split assignments. This column **should contain the values "train", "validate", or "test"** to designate each row's role. When invoking the AutoML API, pass this column to the `split_col` parameter.
# MAGIC
# MAGIC This approach allows you to define custom data splits tailored to your specific requirements. Ensure that the `custom_split` column accurately reflects the intended distribution of your data into training, validation, and test sets. 
# MAGIC
# MAGIC > _Example for understanding the code below: Consider the three values 0.5, 0.8, and 0.91 that are each mapped to three different rows. We will consider the row containing 0.5 as a _train_ data point, while 0.8 is considered a _validation_ data point and 0.91 as a _test_ data point. Basically, values in the interval [0, 0.79] belong to the training dataset, values between [0.8, 0.89] belong to the validation set, and values between [0.9, 1.0] belong to the test set._

# COMMAND ----------

from pyspark.sql.functions import when, rand

dataset = spark.read.table("customer_churn")

seed = 42 # define your seed here for reproduction
train_ratio, validate_ratio, test_ratio = 0.8, 0.1, 0.1 # define your preferred ratios here

dataset = dataset.withColumn("random", rand(seed=seed))
dataset = dataset.withColumn("custom_split", when(dataset.random < train_ratio, "train")
                                    .when(dataset.random < 1-test_ratio, "validate")
                                    .otherwise("test"))
dataset = dataset.drop("random")
display(dataset)

# COMMAND ----------

# MAGIC %md
# MAGIC  **Further Reading: Stratified Sampling with AutoML**
# MAGIC
# MAGIC Stratified sampling ensures that the distribution of a categorical variable (e.g., target labels) is preserved across the training, validation, and test sets. This is particularly useful when dealing with imbalanced datasets.
# MAGIC
# MAGIC 1. **Identify the Stratification Column** â€“ Choose a categorical variable to maintain proportions across dataset splits.
# MAGIC
# MAGIC 2. **Compute Class Proportions** â€“ Determine the distribution of each category in the dataset.
# MAGIC
# MAGIC 3. **Calculate Sample Sizes** â€“ Apply the desired split ratios to compute the exact number of records per class for each split.
# MAGIC
# MAGIC 4. **Perform Stratified Sampling** â€“ Split each category proportionally into training, validation, and test sets.
# MAGIC
# MAGIC 5. **Assign Labels and Combine Splits** â€“ Label the subsets accordingly and merge them into the final dataset.
# MAGIC
# MAGIC 6. **Validate Class Distribution** â€“ Ensure each split maintains the original class proportions.
# MAGIC
# MAGIC **Sample Code**
# MAGIC
# MAGIC ```from pyspark.sql.functions import count, lit, col, round
# MAGIC
# MAGIC # Load dataset
# MAGIC dataset = spark.read.table("customer_churn")
# MAGIC
# MAGIC # Define stratification column
# MAGIC stratify_col = "Gender"
# MAGIC
# MAGIC # Define split ratios
# MAGIC train_ratio, validate_ratio, test_ratio = 0.8, 0.1, 0.1
# MAGIC seed = 42
# MAGIC
# MAGIC # Step 1: Compute class counts and original distribution
# MAGIC class_counts = dataset.groupBy(stratify_col).agg(count("*").alias("count"))
# MAGIC
# MAGIC original_distribution = (
# MAGIC     class_counts.withColumn("percentage", round((col("count") / dataset.count()) * 100, 2))
# MAGIC     .withColumn("dataset", lit("original"))
# MAGIC )
# MAGIC
# MAGIC # Step 2: Perform stratified sampling
# MAGIC train_df = dataset.sampleBy(stratify_col, {row[stratify_col]: train_ratio for row in class_counts.collect()}, seed)
# MAGIC validate_df = dataset.subtract(train_df).sampleBy(
# MAGIC     stratify_col, {row[stratify_col]: validate_ratio / (validate_ratio + test_ratio) for row in class_counts.collect()}, seed
# MAGIC )
# MAGIC test_df = dataset.subtract(train_df).subtract(validate_df)
# MAGIC
# MAGIC # Assign split labels
# MAGIC train_df = train_df.withColumn("custom_split", lit("train"))
# MAGIC validate_df = validate_df.withColumn("custom_split", lit("validate"))
# MAGIC test_df = test_df.withColumn("custom_split", lit("test"))
# MAGIC
# MAGIC # Combine datasets efficiently
# MAGIC final_dataset = train_df.unionByName(validate_df).unionByName(test_df)
# MAGIC
# MAGIC # Step 4: Validate stratification with correct percentage calculation
# MAGIC def validate_distribution(df, split_name):
# MAGIC     total_split_count = df.count()
# MAGIC     return (
# MAGIC         df.groupBy(stratify_col)
# MAGIC         .agg(count("*").alias("count"))
# MAGIC         .withColumn("dataset", lit(split_name))
# MAGIC         .withColumn("percentage", round((col("count") / total_split_count) * 100, 2))
# MAGIC     )
# MAGIC
# MAGIC # Compute distributions
# MAGIC train_dist = validate_distribution(train_df, "train")
# MAGIC validate_dist = validate_distribution(validate_df, "validate")
# MAGIC test_dist = validate_distribution(test_df, "test")
# MAGIC
# MAGIC # **Ensure Schema Consistency Before Union**
# MAGIC columns_order = ["Gender", "count", "percentage", "dataset"]
# MAGIC
# MAGIC original_distribution = original_distribution.select(*columns_order)
# MAGIC train_dist = train_dist.select(*columns_order)
# MAGIC validate_dist = validate_dist.select(*columns_order)
# MAGIC test_dist = test_dist.select(*columns_order)
# MAGIC
# MAGIC # Combine all distributions (original + splits)
# MAGIC distribution_comparison = original_distribution.unionByName(train_dist).unionByName(validate_dist).unionByName(test_dist)
# MAGIC
# MAGIC # Display the final distribution comparison
# MAGIC display(distribution_comparison)
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC ### Start an Experiment
# MAGIC
# MAGIC Now that we have **feature lookups** and **custom splits column** ready, we can continue to setup an AutoML experiment.

# COMMAND ----------

from databricks import automl
from datetime import datetime

automl_run = automl.classify(
    dataset = dataset,
    target_col = "Churn",
    split_col = "custom_split",
    exclude_cols = ["CustomerID"], # Exclude columns as needed
    timeout_minutes = 5,
    feature_store_lookups = feauture_store_lookups
)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Search for the Best Run
# MAGIC
# MAGIC The search for the best run in this experiment, we need to first **get the experiment ID** and then **search for the runs** by experiment.

# COMMAND ----------

import mlflow
# Get the experiment path by experiment ID
exp_path = mlflow.get_experiment(automl_run.experiment.experiment_id).name
# Find the most recent experiment in the AutoML folder
filter_string=f'name LIKE "{exp_path}"'
automl_experiment_id = mlflow.search_experiments(
  filter_string=filter_string,
  max_results=1,
  order_by=["last_update_time DESC"])[0].experiment_id

# COMMAND ----------

from mlflow.entities import ViewType

# Find the best run ...
automl_runs_pd = mlflow.search_runs(
  experiment_ids=[automl_experiment_id],
  filter_string=f"attributes.status = 'FINISHED'",
  run_view_type=ViewType.ACTIVE_ONLY,
  order_by=["metrics.val_f1_score DESC"]
)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC **Print information about the best trial from the AutoML experiment.**
# MAGIC

# COMMAND ----------

print(automl_run.best_trial)

# COMMAND ----------

# MAGIC %md
# MAGIC **Explanation**
# MAGIC
# MAGIC
# MAGIC - **`print(automl_run.best_trial)`**: This prints information about the best trial or run from the AutoML experiment.
# MAGIC
# MAGIC     - **Model:** Specifies the machine learning model that performed the best. 
# MAGIC
# MAGIC     - **Model path:** The MLflow artifact URL of the model trained in this trial.
# MAGIC
# MAGIC     - **Preprocessors:** Description of the preprocessors run before training the model.
# MAGIC
# MAGIC     - **Training duration:** Displays the duration it took to train the best model.
# MAGIC
# MAGIC     - **Evaluation metric score:** Shows the value of the evaluation metric used to determine the best model. 
# MAGIC
# MAGIC     - **Evaluation metric:** Score of primary metric, evaluated for the validation dataset.

# COMMAND ----------

# MAGIC %md
# MAGIC **Import notebooks for other runs in AutoML.**
# MAGIC
# MAGIC For classification and regression experiments, AutoML generated notebooks for data exploration and the best trial in your experiment are automatically imported to your workspace. Generated notebooks for other experiment trials are saved as MLflow artifacts on DBFS instead of auto-imported into your workspace. 
# MAGIC
# MAGIC For all trials besides the best trial, the **`notebook_path`** and **`notebook_url`** in the TrialInfo Python API are not set. If you need to use these notebooks, you can manually import them into your workspace with the AutoML experiment UI or the **`automl.import_notebook`** Python API.
# MAGIC
# MAGIC **ðŸš¨ Notice:** `destination_path` takes Workspace as root.

# COMMAND ----------

# Create the Destination path for storing the best run notebook
destination_path = f"/Users/{DA.username}/imported_notebooks/demo-3.1-{datetime.now().strftime('%Y%m%d%H%M%S')}"

# Get the path and url for the generated notebook
result = automl.import_notebook(automl_run.trials[1].artifact_uri, destination_path)
print(f"The notebook is imported to: {result.path}")
print(f"The notebook URL           : {result.url}")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ## Conclusion
# MAGIC
# MAGIC In this demo, we show how to use AutoML UI and AutoML API for creating classification model and how we can retrieve the best run and access the generated notebook, and how we can modify the parameters of the best model. 
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC &copy; 2025 Databricks, Inc. All rights reserved.<br/>
# MAGIC Apache, Apache Spark, Spark and the Spark logo are trademarks of the 
# MAGIC <a href="https://www.apache.org/">Apache Software Foundation</a>.<br/>
# MAGIC <br/><a href="https://databricks.com/privacy-policy">Privacy Policy</a> | 
# MAGIC <a href="https://databricks.com/terms-of-use">Terms of Use</a> | 
# MAGIC <a href="https://help.databricks.com/">Support</a>
